{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf65594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import exists\n",
    "import json\n",
    "from pprint import pprint as pprint\n",
    "from typing import List, Optional\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from nltk import data\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "Image.MAX_IMAGE_PIXELS = 1000000000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from collections import Counter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d85be909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be modified from https://doi.org/10.18653/v1/2023.acl-long.88 \n",
    "from nltk.corpus import wordnet as wn\n",
    "from wiktionaryparser import WiktionaryParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "348a5f61-ca8c-4e4e-ba7c-942e81b35f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_MODEL = \"ViT-B/32\"  # (ViT-B/32, ViT-L/14 /mnt/model/ViT-B-32.pt)\n",
    "dictionary_type = 'compensate' # GPT_gen (DG or CADG), compensate (WN+DG or WN+CADG), wordnet (WN)\n",
    "d_split = 'train'\n",
    "GPT_def_path = 'text/GPT_Context_Definitions.json' # definition path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77836e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CLIP_model, preprocess = clip.load(CLIP_MODEL, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcf578e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(path):\n",
    "    img_files = os.listdir(path)\n",
    "    imgs = {}\n",
    "    for file in tqdm(img_files):\n",
    "        file_path = os.path.join(path, file)\n",
    "        img = preprocess(Image.open(file_path)).unsqueeze(0)\n",
    "        imgs[file] = img\n",
    "    return imgs\n",
    "\n",
    "if d_split == 'train':\n",
    "    image_path = \"data/train_v1/train_images_v1\"\n",
    "    data_file_path = \"data/train_v1/train.data.v1.txt\"\n",
    "    gold_file_path = \"data/train_v1/train.gold.v1.txt\"\n",
    "    image_dict_path = 'temp/img_dict_train.pkl'\n",
    "\n",
    "if os.path.isfile(image_dict_path):\n",
    "    img_dict = pickle.load(open(image_dict_path,'rb'))\n",
    "else:\n",
    "    img_dict = image_loader(image_path,preprocess)\n",
    "    pickle.dump(img_dict, open(image_dict_path,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bac1a96e-633e-49f2-935e-89a76893c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_definitions(object):\n",
    "    def __init__(self, GPT_def_path):\n",
    "        temp_dict = json.load(open(GPT_def_path))\n",
    "        \n",
    "        GPT_dict = {}\n",
    "        for key in temp_dict.keys():\n",
    "            for k in temp_dict[key]:\n",
    "                 GPT_dict[k] = []\n",
    "        for key in temp_dict.keys():\n",
    "            for k in temp_dict[key]:\n",
    "                 GPT_dict[k].append(temp_dict[key][k])\n",
    "        self.GPT_dict = GPT_dict\n",
    "        \n",
    "    def get_senses(self, target_word):\n",
    "\n",
    "        return self.GPT_dict[target_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8913fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary_wrapper(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wn = wn\n",
    "        self.wiktionary_parser = WiktionaryParser()\n",
    "        self.GPT_definitions = GPT_definitions(GPT_def_path)\n",
    "        \n",
    "    def get_wn_definitions(self, target_word):\n",
    "        sense_definitions = []\n",
    "        target_senses = self.wn.synsets(target_word)\n",
    "        for synset in target_senses:\n",
    "            sense_definition = synset.definition().split(';')[0]\n",
    "            sense_definitions.append(sense_definition)\n",
    "        sense_definitions = list(set(sense_definitions))\n",
    "        \n",
    "        return sense_definitions\n",
    "        \n",
    "    def get_wiktionary_definitions(self, target_word, lang):\n",
    "        parser = self.wiktionary_parser\n",
    "        sense_definitions = []\n",
    "        \n",
    "        target_senses = parser.fetch(target_word, lang)\n",
    "        for synset in target_senses:\n",
    "            for polysemy in synset['definitions']:\n",
    "                for sense in polysemy['text'][1:]:\n",
    "                    sense_definition = sense.split(';')[0]\n",
    "                sense_definitions.append(sense_definition)\n",
    "        sense_definitions = list(set(sense_definitions))\n",
    "        \n",
    "        return sense_definitions\n",
    "    \n",
    "    def get_GPT_definitions(self, target_word):\n",
    "        return self.GPT_definitions.get_senses(target_word)\n",
    "    \n",
    "    def get_definitions(self, target_word, dictionary_type = \"wordnet\", lang='english'):\n",
    "        if dictionary_type == 'wordnet':\n",
    "            sense_definitions = self.get_wn_definitions(target_word)\n",
    "        elif dictionary_type == 'GPT_gen':\n",
    "            sense_definitions = self.get_GPT_definitions(target_word)\n",
    "        elif dictionary_type == 'compensate':\n",
    "            sense_definitions = self.get_wn_definitions(target_word)\n",
    "            if len(sense_definitions) == 0:\n",
    "                sense_definitions += self.get_GPT_definitions(target_word)\n",
    "        return sense_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b285c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def data_loader(data_file_path, dictionary, dictionary_type=\"wordnet\", gold_file_path = None):\n",
    "\n",
    "    def target_word_preprocessing(target_word):\n",
    "        return target_word\n",
    "\n",
    "    text_data = {}\n",
    "    fin_data = open(data_file_path,encoding='utf-8')\n",
    "    candidate_lens = []\n",
    "    for data_index, line in tqdm(enumerate(fin_data)):\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "\n",
    "        cols = line.split('\\t')\n",
    "        target_word = cols[0]; target_word = target_word_preprocessing(target_word)\n",
    "        context = cols[1]\n",
    "        candidates = cols[2:]\n",
    "\n",
    "\n",
    "        sense_definitions = dictionary.get_definitions(target_word, dictionary_type)\n",
    "        wordnet_definitions = dictionary.get_definitions(target_word, 'wordnet')\n",
    "\n",
    "        text_data[data_index] = {'target_word': target_word,\n",
    "                                 'sense_definitions': sense_definitions,\n",
    "                                 'wordnet_definitions': wordnet_definitions,\n",
    "                                 'context': context,\n",
    "                                 'candidates': candidates}\n",
    "\n",
    "        candidate_lens.append(len(candidates))\n",
    "    fin_data.close()\n",
    "\n",
    "\n",
    "    if gold_file_path:\n",
    "        fin_gold = open(gold_file_path)\n",
    "        for gold_index, line in enumerate(fin_gold):\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "\n",
    "            gold = line\n",
    "            text_data[gold_index]['gold'] = gold\n",
    "    print(np.mean(candidate_lens))\n",
    "    return text_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d2a98a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader_gpt3_5(data_file_path, dictionary, dictionary_type=\"wordnet\", gold_file_path = None):\n",
    "    \n",
    "    def target_word_preprocessing(target_word):\n",
    "        return target_word\n",
    "\n",
    "    text_data = {}\n",
    "    de = json.load(open('text/gpt3_5_train.json'))\n",
    "    num = 0\n",
    "\n",
    "    for i in de:\n",
    "        if 'definition'  in de[i][0][0]:\n",
    "            de[i] = [[' ']]\n",
    "        if len(de[i])!=5:\n",
    "            for j in range(len(de[i]),5):\n",
    "                de[i].append([' '])\n",
    "        for j in range(5):\n",
    "            if len(de[i][j])==0:\n",
    "                de[i][j] = [' ']\n",
    "        num +=  len(de[i])\n",
    "\n",
    "    for i in de:\n",
    "        [sense_definition[0] for sense_definition in de[i]]\n",
    "\n",
    "    print(num/len(de))\n",
    "    fin_data = open(data_file_path,encoding='utf-8')\n",
    "    candidate_lens = []\n",
    "\n",
    "    for data_index, line in tqdm(enumerate(fin_data)):\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "\n",
    "        cols = line.split('\\t')\n",
    "        target_word = cols[0]; target_word = target_word_preprocessing(target_word)\n",
    "        context = cols[1]\n",
    "        candidates = cols[2:]\n",
    "\n",
    "        sense_definitions = de[target_word]\n",
    "        wordnet_definitions = dictionary.get_definitions(target_word, 'wordnet')\n",
    "        text_data[data_index] = {'target_word': target_word,\n",
    "                                 'sense_definitions': sense_definitions,\n",
    "                                 'wordnet_definitions': wordnet_definitions,\n",
    "                                 'context': context,\n",
    "                                 'candidates': candidates}\n",
    "\n",
    "        candidate_lens.append(len(candidates))\n",
    "\n",
    "    fin_data.close()\n",
    "    if gold_file_path:\n",
    "        fin_gold = open(gold_file_path)\n",
    "        for gold_index, line in enumerate(fin_gold):\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            gold = line\n",
    "            text_data[gold_index]['gold'] = gold\n",
    "    print(np.mean(candidate_lens))\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def data_loader_gpt4(data_file_path, dictionary, dictionary_type=\"wordnet\", gold_file_path = None):\n",
    "\n",
    "    def target_word_preprocessing(target_word):\n",
    "        return target_word\n",
    "\n",
    "\n",
    "    text_data = {}\n",
    "    de = json.load(open('text/gpt4_train.json'))\n",
    "    num = 0\n",
    "\n",
    "\n",
    "    for i in de:\n",
    "        if len(de[i])!=5:\n",
    "            for j in range(len(de[i]),5):\n",
    "                de[i].append([' '])\n",
    "        for j in range(5):\n",
    "            if len(de[i][j])==0:\n",
    "                de[i][j] = [' ']\n",
    "        if 'definition'  in de[i][0][0] or 'sorry'  in de[i][0][0]:\n",
    "            de[i] = [[' ']]\n",
    "        num +=  len(de[i])\n",
    "\n",
    "    for i in de:\n",
    "        [sense_definition[0] for sense_definition in de[i]]\n",
    "\n",
    "    print(num/len(de))\n",
    "    fin_data = open(data_file_path,encoding='utf-8')\n",
    "    candidate_lens = []\n",
    "    for data_index, line in tqdm(enumerate(fin_data)):\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "\n",
    "        cols = line.split('\\t')\n",
    "        target_word = cols[0]; target_word = target_word_preprocessing(target_word)\n",
    "        context = cols[1]\n",
    "        candidates = cols[2:]\n",
    "\n",
    "\n",
    "        if target_word not in de.keys():\n",
    "            sense_definitions = dictionary.get_definitions(target_word, dictionary_type)\n",
    "        else:\n",
    "            sense_definitions = de[target_word]\n",
    "\n",
    "        wordnet_definitions = dictionary.get_definitions(target_word, 'wordnet')\n",
    "        text_data[data_index] = {'target_word': target_word,\n",
    "                                 'sense_definitions': sense_definitions,\n",
    "                                 'wordnet_definitions': wordnet_definitions,\n",
    "                                 'context': context,\n",
    "                                 'candidates': candidates}\n",
    "\n",
    "        candidate_lens.append(len(candidates))\n",
    "    fin_data.close()\n",
    "\n",
    "    if gold_file_path:\n",
    "        fin_gold = open(gold_file_path)\n",
    "        for gold_index, line in enumerate(fin_gold):\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            gold = line\n",
    "            text_data[gold_index]['gold'] = gold\n",
    "    print(np.mean(candidate_lens))\n",
    "    return text_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Q_VWSD_QC(object):\n",
    "    def __init__(self, CLIP_model, CLIP_preprocess):\n",
    "        self.CLIP_model = CLIP_model;\n",
    "        self.CLIP_preprocess = CLIP_preprocess\n",
    "\n",
    "    def code(self, text_data, img_dict, llm= False):\n",
    "\n",
    "        CLIP_model = self.CLIP_model\n",
    "        states1 = []\n",
    "        states2 = []\n",
    "\n",
    "        for data_index in tqdm(range(len(text_data.keys()))):\n",
    "            data = text_data[data_index]\n",
    "            context = data['context']; candidates = data['candidates']\n",
    "            target_word = data['target_word']\n",
    "            context = context.replace(target_word, '\\\"'+target_word+'\\\"')\n",
    "\n",
    "            sense_definitions = data['sense_definitions']\n",
    "            if llm:\n",
    "                sense_definitions = [context + ' : ' + sense_definition[0] for sense_definition in sense_definitions]\n",
    "            else:\n",
    "                sense_definitions = [context + ' : ' + sense_definition for sense_definition in sense_definitions]\n",
    "\n",
    "            if not len(sense_definitions):\n",
    "                sense_definitions += [context]\n",
    "\n",
    "\n",
    "            gold = data['gold']; gold_index = data['candidates'].index(gold)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                    context_text = clip.tokenize([context], truncate = True).to(device)\n",
    "                    definition_text = clip.tokenize(sense_definitions, truncate = True).to(device)\n",
    "\n",
    "                    images = [img_dict[candidate] for candidate in candidates]\n",
    "                    images = torch.stack(images).squeeze().to(device)\n",
    "\n",
    "                    image_features = CLIP_model.encode_image(images)\n",
    "\n",
    "                    text_features = CLIP_model.encode_text(context_text)\n",
    "                    def_features = CLIP_model.encode_text(definition_text)\n",
    "\n",
    "                    text_features = torch.nn.functional.normalize(text_features,p=2,dim=1)\n",
    "                    def_features = torch.nn.functional.normalize(def_features,p=2,dim=1)\n",
    "\n",
    "                    cosine_similarity = F.cosine_similarity(text_features, def_features, dim=1).unsqueeze(dim=0)\n",
    "\n",
    "                    state = torch.matmul(cosine_similarity, def_features)\n",
    "                    image_features = torch.nn.functional.normalize(image_features,p=2,dim=1)\n",
    "                    states1.append(state)\n",
    "                    states2.append(image_features)\n",
    "        states1 = torch.stack(states1, dim=0)\n",
    "        states2 = torch.stack(states2, dim=0)\n",
    "        torch.save(states1, '/mnt/state_gpt4.0_large.pt')\n",
    "        torch.save(states2, '/mnt/project_large.pt')\n",
    "VWSD_CLIP = Q_VWSD_QC(CLIP_model, preprocess)\n",
    "VWSD_CLIP.code(text_data, img_dict,True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63bc8d8d-782c-4f35-a2b0-fd7c0b1ff511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_VWSD_QI(object):\n",
    "    def __init__(self, CLIP_model, CLIP_preprocess):\n",
    "        self.CLIP_model = CLIP_model; \n",
    "        self.CLIP_preprocess = CLIP_preprocess\n",
    "\n",
    "    def calculate_lower_triangle_matrix_2spp(self,tensor):\n",
    "\n",
    "        result_tensor = torch.zeros(int(tensor.shape[0]*(tensor.shape[0]-1)/2), 10)\n",
    "        index = 0\n",
    "        for i in range(tensor.shape[0]-1):\n",
    "            for j in range(i, tensor.shape[0]-1):\n",
    "                result_tensor[index] = tensor[i,:] * tensor[j,:]\n",
    "                index += 1\n",
    "\n",
    "        return result_tensor\n",
    "\n",
    "    def calculate_lower_triangle_matrix_cos(self,vector_tensor):\n",
    "\n",
    "        cosine_similarity_matrix = torch.nn.functional.cosine_similarity(vector_tensor.unsqueeze(1), vector_tensor.unsqueeze(0), dim=2)\n",
    "        mask = torch.tril(torch.ones_like(cosine_similarity_matrix), diagonal=-1)\n",
    "        matrix_no_diagonal = cosine_similarity_matrix * mask\n",
    "        lower_triangle_tensor = matrix_no_diagonal[mask == 1]\n",
    "        return lower_triangle_tensor\n",
    "\n",
    "    def evaluate_posterior(self, text_data, img_dict):\n",
    "        CLIP_model = self.CLIP_model\n",
    "        preds = []\n",
    "        golds = []\n",
    "        answers = []\n",
    "        partial_answers = []\n",
    "        for data_index in tqdm(text_data.keys()):\n",
    "            data = text_data[data_index]\n",
    "            context = data['context']; candidates = data['candidates']\n",
    "            target_word = data['target_word']\n",
    "            context = context.replace(target_word, '\\\"'+target_word+'\\\"')\n",
    "            gold = data['gold']; gold_index = data['candidates'].index(gold)\n",
    "            text = clip.tokenize([context]).to(device)\n",
    "            with torch.no_grad():\n",
    "                images = [img_dict[candidate] for candidate in candidates]\n",
    "                images = torch.stack(images).squeeze().to(device)\n",
    "\n",
    "                logits_per_image, logits_per_text = CLIP_model(images, text)\n",
    "                probs = logits_per_text.softmax(dim=-1).cpu().numpy()\n",
    "                pred = np.argmax(probs[0])\n",
    "\n",
    "                preds.append(data['candidates'][pred])\n",
    "                golds.append(gold)\n",
    "                if pred == gold_index:\n",
    "                    answers.append(1)\n",
    "                else:\n",
    "                    answers.append(0)\n",
    "\n",
    "                sorted_indexes = reversed(np.argsort(probs[0]))\n",
    "\n",
    "                i = 1\n",
    "                for index in sorted_indexes:\n",
    "                    if index == gold_index:\n",
    "                        partial_answers.append(1/i)\n",
    "                        break\n",
    "                    i+=1\n",
    "        return preds, golds, answers, partial_answers\n",
    "\n",
    "\n",
    "    def evaluate_bayesian_posterior(self, text_data, img_dict, llm=False):\n",
    "        CLIP_model = self.CLIP_model\n",
    "        preds = []\n",
    "        golds = []\n",
    "        answers = []\n",
    "        partial_answers = []\n",
    "        for data_index in tqdm(range(len(text_data.keys()))):\n",
    "            data = text_data[data_index]\n",
    "            context = data['context']; candidates = data['candidates']\n",
    "            target_word = data['target_word']\n",
    "            context = context.replace(target_word, '\\\"'+target_word+'\\\"')\n",
    "\n",
    "            sense_definitions = data['sense_definitions']\n",
    "            if llm:\n",
    "                sense_definitions = [context + ' : ' + sense_definition[0] for sense_definition in sense_definitions]\n",
    "            else:\n",
    "                sense_definitions = [context + ' : ' + sense_definition for sense_definition in sense_definitions]\n",
    "\n",
    "            if not len(sense_definitions):\n",
    "                sense_definitions += [context]\n",
    "            gold = data['gold']; gold_index = data['candidates'].index(gold)\n",
    "            with torch.no_grad():\n",
    "                context_text = clip.tokenize([context], truncate = True).to(device)\n",
    "                definition_text = clip.tokenize(sense_definitions, truncate = True).to(device)\n",
    "\n",
    "                images = [img_dict[candidate] for candidate in candidates]\n",
    "                images = torch.stack(images).squeeze().to(device)\n",
    "\n",
    "                text_features = CLIP_model.encode_text(context_text)\n",
    "                def_features = CLIP_model.encode_text(definition_text)\n",
    "                logits_per_definition = torch.matmul(text_features, def_features.T)\n",
    "                prob_dist_definitions = logits_per_definition.softmax(dim=-1)\n",
    "\n",
    "                logits_per_image, logits_per_text = CLIP_model(images, definition_text)\n",
    "                probs_per_text = logits_per_text.softmax(dim=-1)\n",
    "                bayesian_probs = torch.matmul(prob_dist_definitions, probs_per_text).cpu().numpy()\n",
    "                pred = np.argmax(bayesian_probs)\n",
    "                sorted_indexes = reversed(np.argsort(bayesian_probs[0]))\n",
    "\n",
    "                i = 1\n",
    "                for index in sorted_indexes:\n",
    "                    if index == gold_index:\n",
    "                        partial_answers.append(1/i)\n",
    "                        break\n",
    "                    i+=1\n",
    "\n",
    "                preds.append(data['candidates'][pred])\n",
    "                golds.append(gold)\n",
    "                if pred == gold_index:\n",
    "                    answers.append(1)\n",
    "                else:\n",
    "                    answers.append(0)\n",
    "        return preds, golds, answers, partial_answers\n",
    "\n",
    "    def evaluate_QI_posterior(self, text_data, img_dict, llm= False):\n",
    "\n",
    "        CLIP_model = self.CLIP_model\n",
    "        preds = []\n",
    "        golds = []\n",
    "        answers = []\n",
    "        partial_answers = []\n",
    "        for data_index in tqdm(range(len(text_data.keys()))):\n",
    "            data = text_data[data_index]\n",
    "            context = data['context']; candidates = data['candidates']\n",
    "            target_word = data['target_word']\n",
    "            context = context.replace(target_word, '\\\"'+target_word+'\\\"')\n",
    "\n",
    "            sense_definitions = data['sense_definitions']\n",
    "            if llm:\n",
    "                sense_definitions = [context + ' : ' + sense_definition[0] for sense_definition in sense_definitions]\n",
    "            else:\n",
    "                sense_definitions = [context + ' : ' + sense_definition for sense_definition in sense_definitions]\n",
    "\n",
    "            if not len(sense_definitions):\n",
    "                sense_definitions += [context]\n",
    "\n",
    "\n",
    "            gold = data['gold']; gold_index = data['candidates'].index(gold)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                context_text = clip.tokenize([context], truncate = True).to(device)\n",
    "                definition_text = clip.tokenize(sense_definitions, truncate = True).to(device)\n",
    "\n",
    "                images = [img_dict[candidate] for candidate in candidates]\n",
    "                images = torch.stack(images).squeeze().to(device)\n",
    "\n",
    "                image_features = CLIP_model.encode_image(images)\n",
    "\n",
    "                text_features = CLIP_model.encode_text(context_text)\n",
    "                def_features = CLIP_model.encode_text(definition_text)\n",
    "\n",
    "                text_features = torch.nn.functional.normalize(text_features,p=2,dim=1)\n",
    "                def_features = torch.nn.functional.normalize(def_features,p=2,dim=1)\n",
    "\n",
    "                logits_per_definition = torch.matmul(text_features, def_features.T)\n",
    "                prob_dist_definitions =  logits_per_definition**2\n",
    "                sum_prob_dist_definitions = torch.sum(prob_dist_definitions,dim=1)\n",
    "                prob_dist_definitions = prob_dist_definitions / sum_prob_dist_definitions\n",
    "\n",
    "                image_features = torch.nn.functional.normalize(image_features,p=2,dim=1)\n",
    "                logits_per_text = torch.matmul(def_features, image_features.T)\n",
    "                probs_per_text = logits_per_text**2\n",
    "\n",
    "                cos = self.calculate_lower_triangle_matrix_cos(def_features).unsqueeze(1).cuda()\n",
    "                sp1p2 = torch.sqrt(prob_dist_definitions.T) * torch.sqrt(probs_per_text).cuda()\n",
    "                sp1p2p3p4 = torch.sum(2*self.calculate_lower_triangle_matrix_2spp(sp1p2).cuda()*cos,dim=0)\n",
    "                qbayesian_probs = (torch.matmul(prob_dist_definitions, probs_per_text)+sp1p2p3p4).cpu().numpy()\n",
    "\n",
    "                pred = np.argmax(qbayesian_probs)\n",
    "\n",
    "                sorted_indexes = reversed(np.argsort(qbayesian_probs[0]))\n",
    "                i = 1\n",
    "                for index in sorted_indexes:\n",
    "                    if index == gold_index:\n",
    "                        partial_answers.append(1/i)\n",
    "                        break\n",
    "                    i+=1\n",
    "\n",
    "                preds.append(data['candidates'][pred])\n",
    "                golds.append(gold)\n",
    "                if pred == gold_index:\n",
    "                    answers.append(1)\n",
    "                else:\n",
    "                    answers.append(0)\n",
    "\n",
    "\n",
    "        return preds, golds, answers, partial_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_data = data_loader_gpt3_5(data_file_path,\n",
    "                        dictionary,\n",
    "                        dictionary_type,\n",
    "                        gold_file_path=gold_file_path)\n",
    "VWSD_CLIP = Q_VWSD_QI(CLIP_model, preprocess)\n",
    "p_preds, p_golds, p_answers, p_partial_answers = VWSD_CLIP.evaluate_posterior(text_data, img_dict)\n",
    "print(\"Accuracy:\", \"%.2f\" % (np.mean(p_answers) * 100))\n",
    "print(\"MRR:\", \"%.2f\" % (np.mean(p_partial_answers) * 100))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index = 0\n",
    "pb_sense_nums_w = []\n",
    "pb_sense_nums_r = []\n",
    "for t, p, g in zip(text_data, p_preds, p_golds):\n",
    "    if p != g:\n",
    "        #print(t, text_data[t]['context'], p, g, len(text_data[t]['sense_definitions']))\n",
    "        pb_sense_nums_w.append(len(text_data[t]['wordnet_definitions']))\n",
    "    else:\n",
    "        pb_sense_nums_r.append(len(text_data[t]['wordnet_definitions']))\n",
    "    index += 1\n",
    "print(sorted(Counter(pb_sense_nums_w).items()))\n",
    "print(sorted(Counter(pb_sense_nums_r).items()))\n",
    "\n",
    "right_when_zero = sorted(Counter(pb_sense_nums_r).items())[0][1]\n",
    "wrong_when_zero = sorted(Counter(pb_sense_nums_w).items())[0][1]\n",
    "\n",
    "right_when_one = sorted(Counter(pb_sense_nums_r).items())[1][1]\n",
    "wrong_when_one = sorted(Counter(pb_sense_nums_w).items())[1][1]\n",
    "\n",
    "right_when_over_one = 0\n",
    "wrong_when_over_one = 0\n",
    "\n",
    "for s, c in sorted(Counter(pb_sense_nums_w).items()):\n",
    "    if s > 1: wrong_when_over_one += c\n",
    "for s, c in sorted(Counter(pb_sense_nums_r).items()):\n",
    "    if s > 1: right_when_over_one += c\n",
    "\n",
    "print('Hits@1 |D^t|==0: %.2f' % (right_when_zero / (right_when_zero + wrong_when_zero) * 100))\n",
    "print('Hits@1 |D^t|==1: %.2f' % (right_when_one / (right_when_one + wrong_when_one) * 100))\n",
    "print('Hits@1 |D^t|>1: %.2f' % (right_when_over_one / (right_when_over_one + wrong_when_over_one) * 100))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_data = data_loader_gpt4(data_file_path,\n",
    "                        dictionary,\n",
    "                        dictionary_type,\n",
    "                        gold_file_path = gold_file_path)\n",
    "VWSD_CLIP = Q_VWSD_QI(CLIP_model, preprocess)\n",
    "p_preds, p_golds, p_answers, p_partial_answers =  VWSD_CLIP.evaluate_QI_posterior(text_data, img_dict,True)\n",
    "print(\"Accuracy:\", \"%.2f\"%(np.mean(p_answers)*100))\n",
    "print(\"MRR:\", \"%.2f\"%(np.mean(p_partial_answers)*100))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index = 0\n",
    "pb_sense_nums_w = []\n",
    "pb_sense_nums_r = []\n",
    "for t, p, g in zip(text_data, p_preds, p_golds):\n",
    "    if p != g:\n",
    "        #print(t, text_data[t]['context'], p, g, len(text_data[t]['sense_definitions']))\n",
    "        pb_sense_nums_w.append(len(text_data[t]['wordnet_definitions']))\n",
    "    else:\n",
    "        pb_sense_nums_r.append(len(text_data[t]['wordnet_definitions']))\n",
    "    index+=1\n",
    "print(sorted(Counter(pb_sense_nums_w).items()))\n",
    "print(sorted(Counter(pb_sense_nums_r).items()))\n",
    "\n",
    "right_when_zero = sorted(Counter(pb_sense_nums_r).items())[0][1]\n",
    "wrong_when_zero = sorted(Counter(pb_sense_nums_w).items())[0][1]\n",
    "\n",
    "right_when_one = sorted(Counter(pb_sense_nums_r).items())[1][1]\n",
    "wrong_when_one = sorted(Counter(pb_sense_nums_w).items())[1][1]\n",
    "\n",
    "right_when_over_one = 0\n",
    "wrong_when_over_one = 0\n",
    "\n",
    "for s, c in sorted(Counter(pb_sense_nums_w).items()):\n",
    "     if s > 1: wrong_when_over_one += c\n",
    "for s, c in sorted(Counter(pb_sense_nums_r).items()):\n",
    "     if s > 1: right_when_over_one += c\n",
    "\n",
    "print('Hits@1 |D^t|==0: %.2f'%(right_when_zero/(right_when_zero + wrong_when_zero)*100))\n",
    "print('Hits@1 |D^t|==1: %.2f'%(right_when_one/(right_when_one + wrong_when_one)*100))\n",
    "print('Hits@1 |D^t|>1: %.2f'%(right_when_over_one/(right_when_over_one + wrong_when_over_one)*100))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
